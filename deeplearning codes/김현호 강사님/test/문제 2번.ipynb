{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI CartPole-v0 환경을 사용하여, 처음으로 총보상이 200이 되었을 때 훈련을 멈추고 그때까지 훈련된 Q 값으로 CartPole을 시연하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='linear')) # 0 or 1\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "memory = collections.deque(maxlen = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    action = env.action_space.sample() # np.random.randint(2)\n",
    "    s2, r, done, _ = env.step(action)\n",
    "    \n",
    "    memory.append([s, action, r, done, s2])\n",
    "    \n",
    "    s = env.reset() if done else s2 # done 이면 리셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.01774083,  0.0139485 ,  0.00890822,  0.01286946]),\n",
       " 0,\n",
       " 1.0,\n",
       " False,\n",
       " array([-0.01746186, -0.18130007,  0.00916561,  0.30834967])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.01895414, -0.96563078,  0.0162453 ,  1.27665253]),\n",
       " 0,\n",
       " 1.0,\n",
       " False,\n",
       " array([-0.03826676, -1.16095607,  0.04177835,  1.57437772])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(memory[0], memory[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 # 처음에는 탐험만 수행한다. (랜덤 행동)\n",
    "gamma = 0.99 # 감쇠율 (discount factor, 미래 보상을 얼마나 중요시할 지를 결정)\n",
    "returns = [] # 에피소드 당 총보상값을 저장한다\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Episode: 1, Reward: 62, Epsilon: 0.99398\n",
      "Episode: 2, Reward: 10, Epsilon: 0.99300\n",
      "Episode: 3, Reward: 38, Epsilon: 0.98927\n",
      "Episode: 4, Reward: 16, Epsilon: 0.98770\n",
      "Episode: 5, Reward: 37, Epsilon: 0.98409\n",
      "Episode: 6, Reward: 38, Epsilon: 0.98040\n",
      "Episode: 7, Reward: 15, Epsilon: 0.97894\n",
      "Episode: 8, Reward: 68, Epsilon: 0.97238\n",
      "Episode: 9, Reward: 14, Epsilon: 0.97103\n",
      "Episode: 10, Reward: 22, Epsilon: 0.96892\n",
      "Episode: 11, Reward: 21, Epsilon: 0.96691\n",
      "Episode: 12, Reward: 16, Epsilon: 0.96538\n",
      "Episode: 13, Reward: 17, Epsilon: 0.96375\n",
      "Episode: 14, Reward: 41, Epsilon: 0.95985\n",
      "Episode: 15, Reward: 19, Epsilon: 0.95805\n",
      "Episode: 16, Reward: 21, Epsilon: 0.95606\n",
      "Episode: 17, Reward: 20, Epsilon: 0.95417\n",
      "Episode: 18, Reward: 21, Epsilon: 0.95219\n",
      "Episode: 19, Reward: 24, Epsilon: 0.94993\n",
      "Episode: 20, Reward: 86, Epsilon: 0.94188\n",
      "Episode: 21, Reward: 41, Epsilon: 0.93807\n",
      "Episode: 22, Reward: 35, Epsilon: 0.93483\n",
      "Episode: 23, Reward: 22, Epsilon: 0.93279\n",
      "Episode: 24, Reward: 18, Epsilon: 0.93113\n",
      "Episode: 25, Reward: 17, Epsilon: 0.92957\n",
      "Episode: 26, Reward: 16, Epsilon: 0.92810\n",
      "Episode: 27, Reward: 19, Epsilon: 0.92636\n",
      "Episode: 28, Reward: 15, Epsilon: 0.92498\n",
      "Episode: 29, Reward: 11, Epsilon: 0.92398\n",
      "Episode: 30, Reward: 13, Epsilon: 0.92279\n",
      "Episode: 31, Reward: 24, Epsilon: 0.92060\n",
      "Episode: 32, Reward: 30, Epsilon: 0.91787\n",
      "Episode: 33, Reward: 20, Epsilon: 0.91606\n",
      "Episode: 34, Reward: 14, Epsilon: 0.91479\n",
      "Episode: 35, Reward: 26, Epsilon: 0.91244\n",
      "Episode: 36, Reward: 12, Epsilon: 0.91136\n",
      "Episode: 37, Reward: 25, Epsilon: 0.90911\n",
      "Episode: 38, Reward: 10, Epsilon: 0.90821\n",
      "Episode: 39, Reward: 12, Epsilon: 0.90713\n",
      "Episode: 40, Reward: 21, Epsilon: 0.90525\n",
      "Episode: 41, Reward: 12, Epsilon: 0.90418\n",
      "Episode: 42, Reward: 24, Epsilon: 0.90203\n",
      "Episode: 43, Reward: 12, Epsilon: 0.90096\n",
      "Episode: 44, Reward: 10, Epsilon: 0.90007\n",
      "Episode: 45, Reward: 12, Epsilon: 0.89901\n",
      "Episode: 46, Reward: 18, Epsilon: 0.89741\n",
      "Episode: 47, Reward: 24, Epsilon: 0.89528\n",
      "Episode: 48, Reward: 10, Epsilon: 0.89440\n",
      "Episode: 49, Reward: 31, Epsilon: 0.89166\n",
      "Episode: 50, Reward: 29, Epsilon: 0.88911\n",
      "Episode: 51, Reward: 11, Epsilon: 0.88814\n",
      "Episode: 52, Reward: 13, Epsilon: 0.88700\n",
      "Episode: 53, Reward: 31, Epsilon: 0.88428\n",
      "Episode: 54, Reward: 27, Epsilon: 0.88193\n",
      "Episode: 55, Reward: 27, Epsilon: 0.87958\n",
      "Episode: 56, Reward: 39, Epsilon: 0.87619\n",
      "Episode: 57, Reward: 23, Epsilon: 0.87420\n",
      "Episode: 58, Reward: 14, Epsilon: 0.87299\n",
      "Episode: 59, Reward: 11, Epsilon: 0.87204\n",
      "Episode: 60, Reward: 61, Epsilon: 0.86680\n",
      "Episode: 61, Reward: 21, Epsilon: 0.86500\n",
      "Episode: 62, Reward: 25, Epsilon: 0.86287\n",
      "Episode: 63, Reward: 31, Epsilon: 0.86023\n",
      "Episode: 64, Reward: 20, Epsilon: 0.85853\n",
      "Episode: 65, Reward: 31, Epsilon: 0.85590\n",
      "Episode: 66, Reward: 26, Epsilon: 0.85371\n",
      "Episode: 67, Reward: 34, Epsilon: 0.85084\n",
      "Episode: 68, Reward: 31, Epsilon: 0.84824\n",
      "Episode: 69, Reward: 27, Epsilon: 0.84598\n",
      "Episode: 70, Reward: 23, Epsilon: 0.84406\n",
      "Episode: 71, Reward: 23, Epsilon: 0.84214\n",
      "Episode: 72, Reward: 13, Epsilon: 0.84106\n",
      "Episode: 73, Reward: 19, Epsilon: 0.83948\n",
      "Episode: 74, Reward: 22, Epsilon: 0.83766\n",
      "Episode: 75, Reward: 18, Epsilon: 0.83617\n",
      "Episode: 76, Reward: 17, Epsilon: 0.83477\n",
      "Episode: 77, Reward: 19, Epsilon: 0.83320\n",
      "Episode: 78, Reward: 44, Epsilon: 0.82959\n",
      "Episode: 79, Reward: 17, Epsilon: 0.82820\n",
      "Episode: 80, Reward: 26, Epsilon: 0.82607\n",
      "Episode: 81, Reward: 24, Epsilon: 0.82412\n",
      "Episode: 82, Reward: 24, Epsilon: 0.82217\n",
      "Episode: 83, Reward: 32, Epsilon: 0.81957\n",
      "Episode: 84, Reward: 14, Epsilon: 0.81844\n",
      "Episode: 85, Reward: 62, Epsilon: 0.81344\n",
      "Episode: 86, Reward: 41, Epsilon: 0.81015\n",
      "Episode: 87, Reward: 19, Epsilon: 0.80864\n",
      "Episode: 88, Reward: 24, Epsilon: 0.80672\n",
      "Episode: 89, Reward: 26, Epsilon: 0.80465\n",
      "Episode: 90, Reward: 24, Epsilon: 0.80275\n",
      "Episode: 91, Reward: 52, Epsilon: 0.79864\n",
      "Episode: 92, Reward: 42, Epsilon: 0.79533\n",
      "Episode: 93, Reward: 13, Epsilon: 0.79431\n",
      "Episode: 94, Reward: 14, Epsilon: 0.79321\n",
      "Episode: 95, Reward: 26, Epsilon: 0.79118\n",
      "Episode: 96, Reward: 94, Epsilon: 0.78387\n",
      "Episode: 97, Reward: 78, Epsilon: 0.77786\n",
      "Episode: 98, Reward: 47, Epsilon: 0.77426\n",
      "Episode: 99, Reward: 17, Epsilon: 0.77296\n",
      "Episode: 100, Reward: 20, Epsilon: 0.77144\n",
      "Episode: 101, Reward: 37, Epsilon: 0.76862\n",
      "Episode: 102, Reward: 37, Epsilon: 0.76582\n",
      "Episode: 103, Reward: 50, Epsilon: 0.76205\n",
      "Episode: 104, Reward: 65, Epsilon: 0.75718\n",
      "Episode: 105, Reward: 53, Epsilon: 0.75323\n",
      "Episode: 106, Reward: 14, Epsilon: 0.75219\n",
      "Episode: 107, Reward: 93, Epsilon: 0.74532\n",
      "Episode: 108, Reward: 44, Epsilon: 0.74209\n",
      "Episode: 109, Reward: 41, Epsilon: 0.73910\n",
      "Episode: 110, Reward: 22, Epsilon: 0.73749\n",
      "Episode: 111, Reward: 18, Epsilon: 0.73619\n",
      "Episode: 112, Reward: 19, Epsilon: 0.73481\n",
      "Episode: 113, Reward: 64, Epsilon: 0.73018\n",
      "Episode: 114, Reward: 17, Epsilon: 0.72896\n",
      "Episode: 115, Reward: 33, Epsilon: 0.72659\n",
      "Episode: 116, Reward: 51, Epsilon: 0.72295\n",
      "Episode: 117, Reward: 33, Epsilon: 0.72060\n",
      "Episode: 118, Reward: 12, Epsilon: 0.71974\n",
      "Episode: 119, Reward: 10, Epsilon: 0.71904\n",
      "Episode: 120, Reward: 43, Epsilon: 0.71599\n",
      "Episode: 121, Reward: 39, Epsilon: 0.71324\n",
      "Episode: 122, Reward: 38, Epsilon: 0.71058\n",
      "Episode: 123, Reward: 18, Epsilon: 0.70932\n",
      "Episode: 124, Reward: 76, Epsilon: 0.70402\n",
      "Episode: 125, Reward: 67, Epsilon: 0.69939\n",
      "Episode: 126, Reward: 57, Epsilon: 0.69547\n",
      "Episode: 127, Reward: 55, Epsilon: 0.69171\n",
      "Episode: 128, Reward: 52, Epsilon: 0.68817\n",
      "Episode: 129, Reward: 43, Epsilon: 0.68526\n",
      "Episode: 130, Reward: 29, Epsilon: 0.68331\n",
      "Episode: 131, Reward: 25, Epsilon: 0.68163\n",
      "Episode: 132, Reward: 37, Epsilon: 0.67915\n",
      "Episode: 133, Reward: 47, Epsilon: 0.67601\n",
      "Episode: 134, Reward: 47, Epsilon: 0.67289\n",
      "Episode: 135, Reward: 71, Epsilon: 0.66820\n",
      "Episode: 136, Reward: 45, Epsilon: 0.66524\n",
      "Episode: 137, Reward: 47, Epsilon: 0.66217\n",
      "Episode: 138, Reward: 151, Epsilon: 0.65240\n",
      "Episode: 139, Reward: 83, Epsilon: 0.64709\n",
      "Episode: 140, Reward: 14, Epsilon: 0.64619\n",
      "Episode: 141, Reward: 14, Epsilon: 0.64530\n",
      "Episode: 142, Reward: 26, Epsilon: 0.64366\n",
      "Episode: 143, Reward: 98, Epsilon: 0.63748\n",
      "Episode: 144, Reward: 70, Epsilon: 0.63310\n",
      "Episode: 145, Reward: 25, Epsilon: 0.63154\n",
      "Episode: 146, Reward: 50, Epsilon: 0.62844\n",
      "Episode: 147, Reward: 82, Epsilon: 0.62339\n",
      "Episode: 148, Reward: 37, Epsilon: 0.62113\n",
      "Episode: 149, Reward: 104, Epsilon: 0.61480\n",
      "Episode: 150, Reward: 74, Epsilon: 0.61035\n",
      "Episode: 151, Reward: 55, Epsilon: 0.60705\n",
      "Episode: 152, Reward: 84, Epsilon: 0.60206\n",
      "Episode: 153, Reward: 27, Epsilon: 0.60046\n",
      "Episode: 154, Reward: 183, Epsilon: 0.58975\n",
      "Episode: 155, Reward: 31, Epsilon: 0.58796\n",
      "Episode: 156, Reward: 140, Epsilon: 0.57993\n",
      "Episode: 157, Reward: 116, Epsilon: 0.57335\n",
      "Episode: 158, Reward: 74, Epsilon: 0.56920\n",
      "Episode: 159, Reward: 134, Epsilon: 0.56176\n",
      "Episode: 160, Reward: 104, Epsilon: 0.55605\n",
      "Episode: 161, Reward: 75, Epsilon: 0.55197\n",
      "Episode: 162, Reward: 124, Epsilon: 0.54529\n",
      "Episode: 163, Reward: 103, Epsilon: 0.53980\n",
      "Episode: 164, Reward: 131, Epsilon: 0.53291\n",
      "Episode: 165, Reward: 191, Epsilon: 0.52301\n",
      "Episode: 166, Reward: 85, Epsilon: 0.51867\n",
      "Episode: 167, Reward: 191, Epsilon: 0.50905\n",
      "Episode: 168, Reward: 85, Epsilon: 0.50483\n",
      "Episode: 169, Reward: 200, Epsilon: 0.49503\n"
     ]
    }
   ],
   "source": [
    "### 총 10000번의 에피소드를 진행한다.\n",
    "for episode in range(300):\n",
    "    \n",
    "    total_reward = 0 # 에피소스당 총보상값\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for i in range(200): # 한 에피소드당 최대 200번만 행동한다\n",
    "        \n",
    "        ### 탐험 확률 지정 (1 부터 시작해서 점점 낮아지다 최소값은 1%)\n",
    "        epsilon = 0.01 + (1-0.01)*np.exp(-0.0001*count)\n",
    "        count += 1\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() # 랜덤 행동\n",
    "        else:\n",
    "            action = np.argmax(model.predict(s.reshape(1,4))[0]) # Q값이 높은 행동 선택\n",
    "            \n",
    "        s2, r, done, _ = env.step(action)\n",
    "        \n",
    "        total_reward += r\n",
    "        \n",
    "        memory.append([s, action, r, done, s2])\n",
    "        \n",
    "        ### 학습 (배치크기는 32)\n",
    "        indices = np.random.choice(len(memory), 32, replace=False)\n",
    "        samples = [memory[i] for i in indices]\n",
    "        \n",
    "        X = np.zeros([32,4]) # 입력 상태값\n",
    "        y = np.zeros([32,2]) # 목표 Q값\n",
    "        \n",
    "        for i, sample in enumerate(samples): # sample -> [s,a,r,done,s2]\n",
    "            X[i] = sample[0]\n",
    "            y[i] = model.predict(sample[0].reshape(1,4))[0]\n",
    "            \n",
    "            if sample[3] == True: # done\n",
    "                y[i][sample[1]] = sample[2]\n",
    "            else:\n",
    "                y[i][sample[1]] = sample[2] + gamma*np.max(model.predict(sample[-1].reshape(1,4))[0])\n",
    "                \n",
    "        model.fit(X, y, epochs=1, verbose=False)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            s = s2\n",
    "            \n",
    "    print('Episode: %d, Reward: %d, Epsilon: %.5f' % (episode+1, total_reward, epsilon))\n",
    "    returns.append(total_reward)\n",
    "    if total_reward == 200 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Count: 200\n",
      "[-2.47618562e-01  7.08104694e-05 -2.70725755e-02 -2.54421282e-01] 1.0 True {'TimeLimit.truncated': True}\n",
      "Epoch: 2, Count: 200\n",
      "[1.74797099 1.45630992 0.1496148  0.26134511] 1.0 True {'TimeLimit.truncated': True}\n",
      "Epoch: 3, Count: 200\n",
      "[0.14560026 0.01957706 0.00141913 0.03699403] 1.0 True {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    env.reset()\n",
    "    \n",
    "    done = False\n",
    "    n = 0\n",
    "    while not done:\n",
    "        #a = np.random.randint(2)\n",
    "        a = np.argmax(model.predict(s.reshape(1,4))[0])\n",
    "        s, r, done, info = env.step(a)\n",
    "        n += 1\n",
    "        \n",
    "        \n",
    "        env.render()\n",
    "        #print(s, r, done, info)\n",
    "        \n",
    "    print('Epoch: %d, Count: %d' % (epoch+1, n))\n",
    "    print(s, r, done, info)\n",
    "    #input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
