{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빅데이터 활용 AI 설계\n",
    "# DCGAN : MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAN : Generative Adversarial Networks\n",
    "- DCGAN : Deep Convolutional GAN\n",
    "- Generator(생성자) / Discriminator(판별자)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://skymind.ai/images/wiki/GANs.png' />\n",
    "(출처 : https://skymind.ai/wiki/generative-adversarial-network-gan?fbclid=IwAR0V4kAn9b5GReI9F-OWSUtRPOUgKuWasqPJvuhgL-4ihot_OqYSNAeKd6Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2269d40e0f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVYklEQVR4nO3df5BV5X3H8fdHlGai1h8lKgP4K6Um6CgmBNvoKI6SQSeJITGOm6nVlop/SKtN6tRaR4kdHZr4ozphrKsSlTH+aBKVcWjQ8UdImoZhUaOApRJqcGFHgoqixlLk2z/uWXvZvfe5d3fv7j3P8nnN3OGe8z3nuU9u4ONznvPjKiIwM8vJXu3ugJnZQDm4zCw7Di4zy46Dy8yy4+Ays+w4uMwsOw4uMxs2khZJ2iJpdZ26JN0mab2kFyV9ppl2HVxmNpzuAWYl6mcBk4vXXOD2Zhp1cJnZsImI5cCbiU3OAe6Lil8CB0oa36jdvVvVwWZI8mX6ZsMsIjSU/WfNmhVbt25tattVq1atAT6oWtUZEZ0D+LgJwGtVy93Fup7UTkMKLkmzgFuBMcBdEbFgKO2ZWftt3bqVrq6upraV9EFETBvCx9UK2YYDnEEfKkoaAyykcow6BeiQNGWw7ZlZeUREU68W6AYmVS1PBDY32mkoc1zTgfURsSEidgAPUjleNbPM7dq1q6lXCywB/qw4u/jHwNsRkTxMhKEdKtY6Nj2p70aS5lI5W2BmGWjhaApJDwAzgHGSuoFrgX2Kz/kXYClwNrAeeB/482baHUpwNXVsWkzUdYIn581y0argioiOBvUALh1ou0MJrkEdm5pZ+ZX9OX1DmeNaCUyWdJSkscD5VI5XzSxzIzg5PyiDHnFFxE5J84BlVC6HWBQRa1rWMzNrm7KPuIZ0HVdELKUyuWZmo0REtOqM4bAZ0SvnzSwPo3rEZWajk4PLzLLj4DKzrLT7jGEzHFxm1o8n580sOx5xmVlWfKhoZllycJlZdhxcZpYdB5eZZcW3/JhZljziMrPsOLjMLDsOLjPLjoPLzLLiyXkzy5JHXGaWHQeXmWXHwWVmWfFN1maWJQeXmWXHZxXNLDsecZlZVjzHZWZZcnCZWXYcXGaWHQeXmWXF9yqaWZY84rK2GjNmTLJ+wAEHDOvnz5s3r27t4x//eHLfY445Jlm/9NJLk/Ubb7yxbq2joyO57wcffJCsL1iwIFn/9re/nayX3agOLkmvAtuBD4GdETGtFZ0ys/Yqe3Dt1YI2To+IqQ4ts9Gj91quRq9mSJolaZ2k9ZKurFE/XNIzkp6X9KKksxu16UNFM9tNKyfnJY0BFgIzgW5gpaQlEbG2arOrgYcj4nZJU4ClwJGpdoc64grgCUmrJM2t0/G5krokdQ3xs8xshLRwxDUdWB8RGyJiB/AgcE7fjwN+v3h/ALC5UaNDHXGdHBGbJR0CPCnpPyNi+W49iugEOgEklfvA2cyAAc1xjeszKOks/s33mgC8VrXcDZzUp435VAZAfwXsC5zZ6EOHFFwRsbn4c4ukR6ik6/L0XmZWdgMIrq0N5rdVq/k+yx3APRFxk6Q/ARZLOi4i6h6vDvpQUdK+kvbvfQ98AVg92PbMrByaPUxsMty6gUlVyxPpfyg4B3i4+Oz/AD4GjEs1OpQR16HAI5J62/lBRPxkCO2NWocffniyPnbs2GT985//fLJ+yimn1K0deOCByX2/9rWvJevt1N3dnazfdtttyfrs2bPr1rZv357c91e/+lWy/tOf/jRZz10LL4dYCUyWdBSwCTgf+EafbTYCZwD3SPo0leD6barRQQdXRGwAThjs/mZWXq06qxgROyXNA5YBY4BFEbFG0nVAV0QsAb4F3Cnpb6gcRl4UDZLTl0OYWT+tvAA1IpZSucShet01Ve/XAicPpE0Hl5ntxg8SNLMsObjMLDsOLjPLjoNrDzB16tRk/emnn07Wh/vRMmXV6MzV1Vdfnay/++67yfr9999ft9bT05Pc96233krW161bl6znzA8SNLMsecRlZtlxcJlZdhxcZpYdB5eZZcWT82aWJY+4zCw7Dq49wMaNG5P1N954I1kv83VcK1asSNa3bduWrJ9++ul1azt27Ejuu3jx4mTdho+Dy8yy4puszSxLDi4zy47PKppZdjziMrOseI7LzLLk4DKz7Di49gBvvvlmsn7FFVck61/84heT9eeffz5Zb/QzXSkvvPBCsj5z5sxk/b333kvWjz322Lq1yy67LLmvtY+Dy8yy4nsVzSxLHnGZWXYcXGaWHQeXmWXHwWVmWfHkvJllySMu49FHH03WG/3u4vbt25P1E044oW5tzpw5yX1vvPHGZL3RdVqNrFmzpm5t7ty5Q2rbhk/Zg2uvRhtIWiRpi6TVVesOlvSkpFeKPw8a3m6a2UjqvV+x0atdGgYXcA8wq8+6K4GnImIy8FSxbGajQLOhVergiojlQN97Ws4B7i3e3wt8pcX9MrM2KntwDXaO69CI6AGIiB5Jh9TbUNJcwJMZZhnZ488qRkQn0AkgqdwzfmbW9tFUM5qZ46rldUnjAYo/t7SuS2bWbq08VJQ0S9I6Sesl1ZwPl3SepLWS1kj6QaM2BxtcS4ALi/cXAo8Nsh0zK6FWBZekMcBC4CxgCtAhaUqfbSYDfw+cHBHHApc3arfhoaKkB4AZwDhJ3cC1wALgYUlzgI3A1xv+L7C63nnnnSHt//bbbw9634svvjhZf+ihh5L1ss+F2OC08FBxOrA+IjYASHqQysm9tVXbXAwsjIi3is9ueATXMLgioqNO6YxG+5pZfgZ4y884SV1Vy53FvHavCcBrVcvdwEl92vgjAEn/DowB5kfET1If6ivnzayfAYy4tkbEtERdtZrvs7w3MJnKkd1E4GeSjouIuj+TPtg5LjMbxVo4Od8NTKpanghsrrHNYxHxvxHx38A6KkFWl4PLzPppYXCtBCZLOkrSWOB8Kif3qj0KnA4gaRyVQ8cNqUZ9qGhm/bRqcj4idkqaByyjMn+1KCLWSLoO6IqIJUXtC5LWAh8CV0TEG6l2HVxmtptWX4AaEUuBpX3WXVP1PoBvFq+mOLhGgfnz59etffazn03ue9pppyXrZ555ZrL+xBNPJOuWp7Jf5uLgMrN+yn7Lj4PLzPpxcJlZVnK4ydrBZWb9OLjMLDsOLjPLjs8qmllWPMdlIyL1E2KNHlvz3HPPJet33nlnsv7MM88k611dXXVrCxcuTO5b9n88o1nZv3sHl5n14+Ays+w4uMwsKwN8kGBbOLjMrB+PuMwsOw4uM8uOg8vMsuPgsrb69a9/naxfdNFFyfr3v//9ZP2CCy4YdH3fffdN7nvfffcl6z09Pcm6DY4vQDWzLPmsopllxyMuM8uOg8vMsuI5LjPLkoPLzLLj4DKz7PisopXaI488kqy/8soryfrNN9+crJ9xxhl1azfccENy3yOOOCJZv/7665P1TZs2JetWWw5zXHs12kDSIklbJK2uWjdf0iZJLxSvs4e3m2Y2knrDq9GrXRoGF3APMKvG+lsiYmrxWlqjbmaZKntwNTxUjIjlko4c/q6YWVlkf6iYME/Si8Wh5EH1NpI0V1KXpPoPHzez0uh9kGAzr3YZbHDdDnwSmAr0ADfV2zAiOiNiWkRMG+RnmdkIy/5QsZaIeL33vaQ7gcdb1iMza7tReagoaXzV4mxgdb1tzSw/2Y+4JD0AzADGSeoGrgVmSJoKBPAqcMkw9tHaaPXq9H+TzjvvvGT9S1/6Ut1ao2d9XXJJ+q/V5MmTk/WZM2cm61Zf2UdczZxV7Kix+u5h6IuZlUC7R1PN8JXzZtZP2W/5GcrlEGY2SrVyjkvSLEnrJK2XdGViu3MlhaSGVyA4uMysn1YFl6QxwELgLGAK0CFpSo3t9gf+GljRTP8cXGa2m2ZDq8kR13RgfURsiIgdwIPAOTW2+0fgO8AHzTTq4DKzfgYQXON674wpXnP7NDUBeK1qubtY9xFJJwKTIqLp60E9OW9Dsm3btmR98eLFdWt33XVXct+9907/9Tz11FOT9RkzZtStPfvss8l993QDOKu4tcFdMarV/EdFaS/gFuCipjuHg8vMamjhWcVuYFLV8kRgc9Xy/sBxwLOSAA4Dlkj6ckTUvb/ZwWVmu2nxdVwrgcmSjgI2AecD36j6rLeBcb3Lkp4F/jYVWuA5LjOroVWT8xGxE5gHLANeBh6OiDWSrpP05cH2zyMuM+unlVfOFw8aXdpn3TV1tp3RTJsOLjPrx7f8mFlWeh8kWGYOLjPrxyMuy9rxxx+frJ977rnJ+uc+97m6tUbXaTWydu3aZH358uVDan9P5uAys+w4uMwsOw4uM8uKHyRoZlnyWUUzy45HXGaWHQeXmWXFc1zWdsccc0yyPm/evGT9q1/9arJ+2GGHDbhPzfrwww+T9Z6enmS97PM0ZebgMrPslD30HVxmthsfKppZlhxcZpYdB5eZZcfBZWbZcXCZWVZGxYMEJU0C7qPys0G7gM6IuFXSwcBDwJHAq8B5EfHW8HV1z9XoWqmOjo66tUbXaR155JGD6VJLdHUlf8iF66+/PllfsmRJK7tjVco+4mrmV352At+KiE8DfwxcKmkKcCXwVERMBp4qls1sFGjVr/wMl4bBFRE9EfFc8X47lZ8YmgCcA9xbbHYv8JXh6qSZjayyB9eA5rgkHQmcCKwADo2IHqiEm6RDWt47Mxtx7Q6lZjQdXJL2A34EXB4R7xQ/l93MfnOBuYPrnpm1w6gILkn7UAmt+yPix8Xq1yWNL0Zb44EttfaNiE6gs2in3N+GmQHlv1ex4RyXKkOru4GXI+LmqtIS4MLi/YXAY63vnpm1w2iY4zoZuAB4SdILxbqrgAXAw5LmABuBrw9PF/N36KGHJutTpkxJ1r/3ve8l65/61KcG3KdWWbFiRbL+3e9+t27tscfS/60r+3/1R6t2h1IzGgZXRPwcqDehdUZru2NmZZB9cJnZnsfBZWbZKfthuoPLzHYzKua4zGzP4+Ays+w4uMwsOw6uUeLggw+uW7vjjjuS+06dOjVZP/roowfVp1b4xS9+kazfdNNNyfqyZcuS9d/97ncD7pO1XyuDS9Is4FZgDHBXRCzoU/8m8JdUnkTzW+AvIuI3qTabeayNme1Beh8k2MyrEUljgIXAWcAUoKN4LFa154FpEXE88EPgO43adXCZWT8tvOVnOrA+IjZExA7gQSqPxKr+rGci4v1i8ZfAxEaN+lDRzPoZwKHiOEnVj7LtLB6s0GsC8FrVcjdwUqK9OcC/NfpQB5eZ9TOA4NoaEdMS9Vq3C9ZsXNKfAtOA0xp9qIPLzHbT4gtQu4FJVcsTgc19N5J0JvAPwGkR8T+NGnVwmVk/LQyulcBkSUcBm4DzgW9UbyDpROAOYFZE1HyuX18OLjPrp1X3KkbETknzgGVULodYFBFrJF0HdEXEEuC7wH7AvxZPVt4YEV9OtbvHBNdJJ6XmA+GKK65I1qdPn163NmHChEH1qVXef//9urXbbrstue8NN9yQrL/33nuD6pPlrZXXcUXEUmBpn3XXVL0/c6Bt7jHBZWbN8U3WZpYlB5eZZcfBZWbZ8YMEzSwrnuMysyw5uMwsOw6ukpg9e/aQ6kOxdu3aZP3xxx9P1nfu3Jmsp56ZtW3btuS+ZrU4uMwsOw4uM8tK74MEy8zBZWb9eMRlZtlxcJlZdhxcZpYVX4BqZlkqe3CpUQclTQLuAw4DdlF5GP6tkuYDF1P5HTSAq4rn7qTaKve3YTYKRESt57w3bezYsfGJT3yiqW03b968qsEz54dFMyOuncC3IuI5SfsDqyQ9WdRuiYgbh697ZtYOZR9xNQyuiOgBeor32yW9TOUnh8xsFMphjmtAPwgr6UjgRGBFsWqepBclLZJ0UJ195krq6vPba2ZWYi38Qdhh0XRwSdoP+BFweUS8A9wOfBKYSmVEVvOGuYjojIhp7TgONrPBKXtwNXVWUdI+VELr/oj4MUBEvF5VvxNI3ylsZtko+y0/DUdcqvxe0N3AyxFxc9X68VWbzQZWt757ZjbSmh1tlX3EdTJwAfCSpBeKdVcBHZKmUvk57VeBS4alh2Y24so+Od/MWcWfA7WuC0les2Vm+co+uMxsz+PgMrPsOLjMLCt+kKCZZckjLjPLjoPLzLLj4DKzrLT74tJmOLjMrB8Hl5llx2cVzSw7HnGZWVZymOMa0IMEzWzP0MqnQ0iaJWmdpPWSrqxR/z1JDxX1FcUDS5McXGbWT6uCS9IYYCFwFjCFylNlpvTZbA7wVkT8IXAL8E+N2nVwmVk/u3btaurVhOnA+ojYEBE7gAeBc/pscw5wb/H+h8AZxXMA6xrpOa6twG+qlscV68qorH0ra7/AfRusVvbtiBa0sYxKn5rxsT6/J9EZEZ1VyxOA16qWu4GT+rTx0TYRsVPS28AfkPhORjS4ImK3H2uT1FXWZ9GXtW9l7Re4b4NVtr5FxKwWNldr5NT3GLOZbXbjQ0UzG07dwKSq5YnA5nrbSNobOAB4M9Wog8vMhtNKYLKkoySNBc4HlvTZZglwYfH+XODpaDDz3+7ruDobb9I2Ze1bWfsF7ttglblvQ1LMWc2jMm82BlgUEWskXQd0RcQSKj/Gs1jSeiojrfMbtauyX2hmZtaXDxXNLDsOLjPLTluCq9EtAO0k6VVJL0l6oc/1Ke3oyyJJWyStrlp3sKQnJb1S/HlQifo2X9Km4rt7QdLZberbJEnPSHpZ0hpJlxXr2/rdJfpViu8tJyM+x1XcAvBfwEwqp0FXAh0RsXZEO1KHpFeBaRHR9osVJZ0KvAvcFxHHFeu+A7wZEQuK0D8oIv6uJH2bD7wbETeOdH/69G08MD4inpO0P7AK+ApwEW387hL9Oo8SfG85aceIq5lbAAyIiOX0v56l+vaIe6n8xR9xdfpWChHRExHPFe+3Ay9TuTq7rd9dol82QO0Irlq3AJTp/7wAnpC0StLcdnemhkMjogcq/xCAQ9rcn77mSXqxOJRsy2FsteJJAycCKyjRd9enX1Cy763s2hFcA768f4SdHBGfoXI3+6XFIZE153bgk8BUoAe4qZ2dkbQf8CPg8oh4p519qVajX6X63nLQjuBq5haAtomIzcWfW4BHqBzalsnrxVxJ75zJljb35yMR8XpEfBgRu4A7aeN3J2kfKuFwf0T8uFjd9u+uVr/K9L3loh3B1cwtAG0had9i0hRJ+wJfAFan9xpx1bdHXAg81sa+7KY3FAqzadN3VzwS5W7g5Yi4uarU1u+uXr/K8r3lpC1Xzhene/+Z/78F4PoR70QNko6mMsqCyu1QP2hn3yQ9AMyg8oiR14FrgUeBh4HDgY3A1yNixCfJ6/RtBpXDnQBeBS7pnVMa4b6dAvwMeAnofWjUVVTmk9r23SX61UEJvrec+JYfM8uOr5w3s+w4uMwsOw4uM8uOg8vMsuPgMrPsOLjMLDsOLjPLzv8BwPSZWTTufj8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,:,:,np.newaxis] # X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), dtype('float64'), 1.0, 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train.dtype, X_train.max(), X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 판별자 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, \\\n",
    "    Flatten, Reshape, LeakyReLU, BatchNormalization, Activation\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input((28,28,1))\n",
    "x = LeakyReLU(alpha=0.2)(input)\n",
    "\n",
    "x = Conv2D(32, (5,5), strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(64, (5,5), strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(128, (5,5), strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(256, (5,5), strides=1, padding='same')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = Model(input, x)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.researchgate.net/profile/Sepp_Hochreiter/publication/284579051/figure/fig1/AS:614057178578955@1523414048184/The-rectified-linear-unit-ReLU-the-leaky-ReLU-LReLU-a-01-the-shifted-ReLUs.png' />\n",
    "(출처: https://www.researchgate.net/figure/The-rectified-linear-unit-ReLU-the-leaky-ReLU-LReLU-a-01-the-shifted-ReLUs_fig1_284579051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=RMSprop(lr=2e-4, decay=6e-8), metrics=['acc'])\n",
    "    # decay 값이 클수록 새롭게(처음으로) 변경되는 가중치를 많이 변화시킨다 (디폴트:0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성자 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input2 = Input((100,)) # 생성벡터는 100차원\n",
    "\n",
    "x2 = Dense(7*7*128)(input2) # 랜덤 이미지가 입력됨\n",
    "\n",
    "x2 = Reshape((7,7,128))(x2)\n",
    "\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x2 = Conv2DTranspose(128, (5,5), strides=2, padding='same')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x2 = Conv2DTranspose(64, (5,5), strides=2, padding='same')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x2 = Conv2DTranspose(32, (5,5), strides=1, padding='same')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x2 = Conv2DTranspose(1, (5,5), strides=1, padding='same')(x2)\n",
    "x2 = Activation('sigmoid')(x2)\n",
    "\n",
    "generator = Model(input2, x2)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 적대망 정의\n",
    "- 생성자로 생성한 이미지가 판별자에 의해 '실제이미지' 로 판별되도록 생성자의 가중치를 조절한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 28, 28, 1)         1301505   \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 1080577   \n",
      "=================================================================\n",
      "Total params: 2,382,082\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 1,081,281\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.trainable = False # 모델들을 연결하여 새로운 모델을 만들 때만 적용된다\n",
    "\n",
    "adversarial = Model(input2, discriminator(generator(input2)))\n",
    "adversarial.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial.compile(loss='binary_crossentropy',\n",
    "            optimizer=RMSprop(lr=2e-4*0.5, decay=6e-8*0.5), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련하기\n",
    "- fake images: 0, real images: 1\n",
    "- 1 step 에 10초 정도 소요됨\n",
    "- C:\\khh\\프로그래밍\\CNN\\gan_images 폴더에 'fake_9.png' 형태로 저장함\n",
    "- 생성자 모델을 'gan_mnist_generator_1000.h5' 로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 00001,  [0.6926977, 0.2890625, 1.0251112, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 00002,  [0.6380305, 0.5, 1.0652347, 0.0]\n",
      "Step: 00003,  [0.5411577, 1.0, 1.4819052, 0.0]\n",
      "Step: 00004,  [0.4187925, 0.9609375, 1.3302001, 0.0]\n",
      "Step: 00005,  [0.30871677, 1.0, 2.845601, 0.0]\n",
      "Step: 00006,  [0.25620753, 0.9453125, 1.157366, 0.0]\n",
      "Step: 00007,  [0.19986457, 0.9921875, 2.9595304, 0.0]\n",
      "Step: 00008,  [0.13915533, 0.984375, 1.5469143, 0.0]\n",
      "Step: 00009,  [0.09333084, 0.9921875, 1.1590295, 0.0]\n",
      "Step: 00010,  [0.0525401, 1.0, 0.91476834, 0.015625]\n",
      "Step: 00011,  [0.07001501, 0.9921875, 0.37281543, 1.0]\n",
      "Step: 00012,  [0.05520773, 0.9921875, 0.92688966, 0.03125]\n",
      "Step: 00013,  [0.04486181, 0.984375, 0.29486033, 1.0]\n",
      "Step: 00014,  [0.026731027, 1.0, 0.23420426, 1.0]\n",
      "Step: 00015,  [0.023039985, 1.0, 0.21694629, 1.0]\n",
      "Step: 00016,  [0.02808114, 0.9921875, 0.11981237, 1.0]\n",
      "Step: 00017,  [0.02338395, 0.9921875, 0.14345613, 1.0]\n",
      "Step: 00018,  [0.024605528, 0.9921875, 0.09054917, 1.0]\n",
      "Step: 00019,  [0.024670277, 1.0, 0.123630874, 1.0]\n",
      "Step: 00020,  [0.014267772, 1.0, 0.12170452, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "batch_size = 64\n",
    "logs = [] # d_loss,d_acc,a_loss,a_acc\n",
    "\n",
    "for step in range(20): # 400\n",
    "    indices = np.random.randint(0, len(X_train), size=batch_size)\n",
    "    real_images = X_train[indices]\n",
    "    \n",
    "    gen_vectors = np.random.uniform(-1, 1, size=[batch_size,100])\n",
    "    fake_images = generator.predict(gen_vectors)\n",
    "    \n",
    "    X = np.r_[real_images, fake_images] # [batch_size*2,32,32,1]\n",
    "    y = np.r_[np.ones([batch_size,1]), np.zeros([batch_size,1])] # [batch_size*2,1]\n",
    "        # fake: 0, real: 1\n",
    "        \n",
    "    ###########\n",
    "    d_loss, d_acc = discriminator.train_on_batch(X, y)\n",
    "    \n",
    "    gen_vectors_2 = np.random.uniform(-1, 1, size=[batch_size,100])\n",
    "    ###########\n",
    "    a_loss, a_acc = adversarial.train_on_batch(gen_vectors_2, np.ones([batch_size,1]))\n",
    "            # 타겟값을 모두 1로 놓는다\n",
    "    \n",
    "    logs.append([d_loss,d_acc,a_loss,a_acc])\n",
    "    print('Step: %05d, ' % (step+1), logs[-1])\n",
    "    \n",
    "    if (step+1)%10 == 0:\n",
    "        adversarial.save_weights('gan_mnist.h5')\n",
    "    \n",
    "        img = image.array_to_img(fake_images[0]*255., scale=False)\n",
    "        img.save('gan_images/fake_'+str(step)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('gan_mnist_generator_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 1000번 배치 실행 (이전 400번 + 신규 600번)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 00401, [0.018, 1.000, 0.059, 1.000]\n",
      "Step: 00402, [0.025, 0.992, 0.155, 1.000]\n",
      "Step: 00403, [0.023, 1.000, 0.016, 1.000]\n",
      "Step: 00404, [0.068, 1.000, 5.817, 0.000]\n",
      "Step: 00405, [0.320, 0.883, 0.087, 1.000]\n",
      "Step: 00406, [0.756, 0.516, 6.724, 0.000]\n",
      "Step: 00407, [0.611, 0.773, 2.305, 0.000]\n",
      "Step: 00408, [0.147, 0.945, 1.411, 0.031]\n",
      "Step: 00409, [0.169, 0.953, 1.268, 0.062]\n",
      "Step: 00410, [0.112, 0.977, 1.054, 0.141]\n",
      "Step: 00411, [0.186, 0.945, 1.897, 0.000]\n",
      "Step: 00412, [0.213, 0.930, 1.892, 0.000]\n",
      "Step: 00413, [0.348, 0.914, 3.775, 0.000]\n",
      "Step: 00414, [0.447, 0.875, 1.889, 0.000]\n",
      "Step: 00415, [0.411, 0.773, 6.372, 0.000]\n",
      "Step: 00416, [1.190, 0.617, 1.440, 0.000]\n",
      "Step: 00417, [0.862, 0.500, 4.638, 0.000]\n",
      "Step: 00418, [0.889, 0.688, 2.023, 0.000]\n",
      "Step: 00419, [0.630, 0.445, 2.674, 0.000]\n",
      "Step: 00420, [0.565, 0.852, 2.003, 0.000]\n",
      "Step: 00421, [0.508, 0.578, 2.941, 0.000]\n",
      "Step: 00422, [0.537, 0.898, 1.985, 0.000]\n",
      "Step: 00423, [0.483, 0.570, 2.972, 0.000]\n",
      "Step: 00424, [0.527, 0.875, 1.812, 0.000]\n",
      "Step: 00425, [0.443, 0.609, 2.874, 0.000]\n",
      "Step: 00426, [0.430, 0.906, 1.808, 0.000]\n",
      "Step: 00427, [0.405, 0.773, 2.864, 0.000]\n",
      "Step: 00428, [0.570, 0.867, 1.618, 0.000]\n",
      "Step: 00429, [0.581, 0.484, 3.550, 0.000]\n",
      "Step: 00430, [0.751, 0.672, 0.928, 0.000]\n",
      "Step: 00431, [0.706, 0.500, 2.974, 0.000]\n",
      "Step: 00432, [0.454, 0.875, 1.579, 0.000]\n",
      "Step: 00433, [0.359, 0.930, 2.309, 0.000]\n",
      "Step: 00434, [0.369, 0.969, 1.695, 0.000]\n",
      "Step: 00435, [0.397, 0.891, 2.122, 0.000]\n",
      "Step: 00436, [0.457, 0.938, 1.336, 0.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5978be9cbbf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mgen_vectors_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m###########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0ma_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversarial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_vectors_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_acc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for step in range(400,1000): # 40000\n",
    "    idx = np.random.randint(0, len(X_train), size=batch_size)\n",
    "    real_images = X_train[idx]\n",
    "    \n",
    "    gen_vectors = np.random.uniform(-1, 1, size=[batch_size,100])\n",
    "    fake_images = generator.predict(gen_vectors)\n",
    "    \n",
    "    X = np.r_[real_images, fake_images] # [batch_size*2,32,32,1]\n",
    "    y = np.r_[np.ones([batch_size,1]), np.zeros([batch_size,1])] # [batch_size*2,1]\n",
    "        # fake: 0, real: 1\n",
    "        \n",
    "    ###########\n",
    "    d_loss, d_acc = discriminator.train_on_batch(X, y)\n",
    "    \n",
    "    gen_vectors_2 = np.random.uniform(-1, 1, size=[batch_size,100])\n",
    "    ###########\n",
    "    a_loss, a_acc = adversarial.train_on_batch(gen_vectors_2, np.ones([batch_size,1]))\n",
    "    \n",
    "    logs.append([d_loss,d_acc,a_loss,a_acc])\n",
    "    print('Step: %05d, [%.3f, %.3f, %.3f, %.3f]' % (step+1,d_loss,d_acc,a_loss,a_acc))\n",
    "    \n",
    "    if (step+1)%10 == 0:\n",
    "        adversarial.save_weights('gan_mnist.h5')\n",
    "    \n",
    "        img = image.array_to_img(fake_images[0]*255., scale=False)\n",
    "        img.save('gan_images/fake_'+str(step)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('gan_mnist_generator_1000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gan_mnist_log.npy', logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in logs][::10], 'b-', label='d_loss')\n",
    "plt.plot([i[2] for i in logs][::10], 'r-', label='a_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([i[1] for i in logs][::10], 'b-', label='d_acc')\n",
    "plt.plot([i[3] for i in logs][::10], 'r-', label='a_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과 비교\n",
    "<img src='https://d33wubrfki0l68.cloudfront.net/1439d140302850f4652f4def43d264b7afa39e65/81f84/assets/images/gan-intro/mnistkeras_19_0.png' />\n",
    "(출처: https://www.wouterbulten.nl/blog/tech/getting-started-with-generative-adversarial-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 숫자 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#generator = load_model('gan_mnist_generator_1000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.random.uniform(-1, 1, size=[100,100])\n",
    "images = generator.predict(vectors)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.imshow(images[i].reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets.fashion_mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
